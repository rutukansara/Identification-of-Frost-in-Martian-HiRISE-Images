{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rutuk\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
    "\n",
    "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
    "SEED = 17\n",
    "\n",
    "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
    "data_head_dir = Path('data/data/')\n",
    "\n",
    "# Find all subframe directories\n",
    "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
    "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]\n",
    "\n",
    "# Load train/val/test subframe IDs\n",
    "def load_text_ids(file_path):\n",
    "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "# Load the subframe names for the three data subsets\n",
    "train_ids = load_text_ids('train_source_images.txt')\n",
    "validate_ids = load_text_ids('val_source_images.txt')\n",
    "test_ids = load_text_ids('test_source_images.txt')\n",
    "\n",
    "# Generate a list containing the dataset split for the matching subdirectory names\n",
    "subdir_splits = []\n",
    "for src_id in src_image_ids:\n",
    "    if src_id in train_ids:\n",
    "        subdir_splits.append('train')\n",
    "    elif src_id in validate_ids:\n",
    "        subdir_splits.append('validate')\n",
    "    elif(src_id in test_ids):\n",
    "        subdir_splits.append('test')\n",
    "    else:\n",
    "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
    "        subdir_splits.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:41:05 | WARNING | module_wrapper  | From c:\\Users\\rutuk\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "07:41:05 | WARNING | module_wrapper  | From c:\\Users\\rutuk\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "07:41:17 | WARNING | module_wrapper  | From c:\\Users\\rutuk\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:41:20 | WARNING | module_wrapper  | From c:\\Users\\rutuk\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "07:41:22 | WARNING | module_wrapper  | From c:\\Users\\rutuk\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928/928 [==============================] - 2794s 3s/step - loss: 0.1134 - accuracy: 0.9579 - val_loss: 0.4791 - val_accuracy: 0.8454\n",
      "Epoch 2/20\n",
      "928/928 [==============================] - 2723s 3s/step - loss: 0.0592 - accuracy: 0.9790 - val_loss: 0.5479 - val_accuracy: 0.8331\n",
      "Epoch 3/20\n",
      "928/928 [==============================] - 2499s 3s/step - loss: 0.0499 - accuracy: 0.9822 - val_loss: 0.6111 - val_accuracy: 0.8402\n",
      "Epoch 4/20\n",
      "928/928 [==============================] - 2507s 3s/step - loss: 0.0437 - accuracy: 0.9855 - val_loss: 0.5893 - val_accuracy: 0.8254\n",
      "Epoch 5/20\n",
      "928/928 [==============================] - 2522s 3s/step - loss: 0.0395 - accuracy: 0.9864 - val_loss: 0.7948 - val_accuracy: 0.7803\n",
      "Epoch 6/20\n",
      "928/928 [==============================] - 2520s 3s/step - loss: 0.0355 - accuracy: 0.9871 - val_loss: 0.3709 - val_accuracy: 0.8754\n",
      "Epoch 7/20\n",
      "928/928 [==============================] - 2679s 3s/step - loss: 0.0284 - accuracy: 0.9899 - val_loss: 0.7187 - val_accuracy: 0.8182\n",
      "Epoch 8/20\n",
      "928/928 [==============================] - 2882s 3s/step - loss: 0.0327 - accuracy: 0.9879 - val_loss: 0.8900 - val_accuracy: 0.7718\n",
      "Epoch 9/20\n",
      "928/928 [==============================] - 2931s 3s/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.6597 - val_accuracy: 0.8221\n",
      "Epoch 10/20\n",
      "625/928 [===================>..........] - ETA: 13:41 - loss: 0.0273 - accuracy: 0.9901"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50, VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "def load_and_preprocess_transfer_learning(img_loc, label):\n",
    "    def _inner_function(img_loc, label):\n",
    "        img_loc_str = img_loc.numpy().decode('utf-8')\n",
    "        img = Image.open(img_loc_str).convert('RGB')\n",
    "        img = np.array(img)\n",
    "        img = tf.image.resize(img, [224, 224])  # Adjust size for EfficientNetB0\n",
    "        img = preprocess_input(img)  # Preprocess for EfficientNetB0\n",
    "        label = 1 if label.numpy().decode('utf-8') == 'frost' else 0\n",
    "        return img, label\n",
    "    \n",
    "    X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int64])\n",
    "    X.set_shape([224, 224, 3])  # Adjust shape for EfficientNetB0\n",
    "    y.set_shape([])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_subdir_data_transfer_learning(dir_path, image_size, seed=None):\n",
    "    \"\"\"Helper to create a TF dataset from each image subdirectory for transfer learning\"\"\"\n",
    "    tile_dir = dir_path / Path('tiles')\n",
    "    label_dir = dir_path / Path('labels')\n",
    "    \n",
    "    loc_list = []\n",
    "    \n",
    "    for folder in os.listdir(tile_dir):\n",
    "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
    "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
    "                if file.endswith(\".png\"):\n",
    "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
    "\n",
    "    return loc_list\n",
    "\n",
    "\n",
    "# Adapt the original data preprocessing for transfer learning\n",
    "tf_data_train_transfer, tf_data_test_transfer, tf_data_val_transfer = [], [], []\n",
    "tf_dataset_train_transfer, tf_dataset_test_transfer, tf_dataset_val_transfer = [], [], []\n",
    "\n",
    "for subdir, split in zip(subdirs, subdir_splits):\n",
    "    full_path = data_head_dir / subdir\n",
    "    if split == 'validate':\n",
    "        tf_data_val_transfer.extend(load_subdir_data_transfer_learning(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split == 'train':\n",
    "        tf_data_train_transfer.extend(load_subdir_data_transfer_learning(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split == 'test':\n",
    "        tf_data_test_transfer.extend(load_subdir_data_transfer_learning(full_path, IMAGE_SIZE, SEED))\n",
    "\n",
    "random.shuffle(tf_data_train_transfer)\n",
    "img_list_transfer, label_list_transfer = zip(*tf_data_train_transfer)\n",
    "img_list_t_transfer = tf.convert_to_tensor(img_list_transfer)\n",
    "lb_list_t_transfer = tf.convert_to_tensor(label_list_transfer)\n",
    "buffer_size = 64\n",
    "batch_size = 32\n",
    "tf_dataset_train_transfer = tf.data.Dataset.from_tensor_slices((img_list_t_transfer, lb_list_t_transfer))\n",
    "tf_dataset_train_transfer = tf_dataset_train_transfer.map(load_and_preprocess_transfer_learning,\n",
    "                                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_train_transfer = tf_dataset_train_transfer.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "\n",
    "random.shuffle(tf_data_val_transfer)\n",
    "img_list_transfer, label_list_transfer = zip(*tf_data_val_transfer)\n",
    "img_list_t_transfer = tf.convert_to_tensor(img_list_transfer)\n",
    "lb_list_t_transfer = tf.convert_to_tensor(label_list_transfer)\n",
    "\n",
    "tf_dataset_val_transfer = tf.data.Dataset.from_tensor_slices((img_list_t_transfer, lb_list_t_transfer))\n",
    "tf_dataset_val_transfer = tf_dataset_val_transfer.map(load_and_preprocess_transfer_learning,\n",
    "                                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_val_transfer = tf_dataset_val_transfer.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "\n",
    "random.shuffle(tf_data_test_transfer)\n",
    "img_list_transfer, label_list_transfer = zip(*tf_data_test_transfer)\n",
    "img_list_t_transfer = tf.convert_to_tensor(img_list_transfer)\n",
    "lb_list_t_transfer = tf.convert_to_tensor(label_list_transfer)\n",
    "\n",
    "tf_dataset_test_transfer = tf.data.Dataset.from_tensor_slices((img_list_t_transfer, lb_list_t_transfer))\n",
    "tf_dataset_test_transfer = tf_dataset_test_transfer.map(load_and_preprocess_transfer_learning,\n",
    "                                                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "tf_dataset_test_transfer = tf_dataset_test_transfer.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
    "\n",
    "\n",
    "# Function to build transfer learning model\n",
    "def build_transfer_learning_model(base_model):\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Freeze the pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input size\n",
    "input_size = (224, 224, 3)\n",
    "\n",
    "# Build models for EfficientNetB0, ResNet50, and VGG16\n",
    "resnet50_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_size)\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=input_size)\n",
    "\n",
    "# Build transfer learning models\n",
    "resnet50_transfer_model = build_transfer_learning_model(resnet50_model)\n",
    "vgg16_transfer_model = build_transfer_learning_model(vgg16_model)\n",
    "\n",
    "# Compile models\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "resnet50_transfer_model = compile_model(resnet50_transfer_model)\n",
    "vgg16_transfer_model = compile_model(vgg16_transfer_model)\n",
    "\n",
    "# Function to train transfer learning model\n",
    "def train_transfer_learning_model(model, train_data, val_data, epochs=20):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_data,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "resnet50_transfer_model, resnet50_history = train_transfer_learning_model(\n",
    "    resnet50_transfer_model, tf_dataset_train_transfer, tf_dataset_val_transfer, epochs=epochs\n",
    ")\n",
    "vgg16_transfer_model, vgg16_history = train_transfer_learning_model(\n",
    "    vgg16_transfer_model, tf_dataset_train_transfer, tf_dataset_val_transfer, epochs=epochs\n",
    ")\n",
    "\n",
    "# Evaluate and report metrics\n",
    "def evaluate_and_report(model, test_data):\n",
    "    y_pred = model.predict(test_data)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    y_true = tf.concat([label for _, label in test_data], axis=0).numpy()\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred_binary))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred_binary))\n",
    "\n",
    "# Evaluate transfer learning models\n",
    "\n",
    "print(\"\\nEvaluation for ResNet50 Transfer Learning Model:\")\n",
    "evaluate_and_report(resnet50_transfer_model, tf_dataset_test_transfer)\n",
    "\n",
    "print(\"\\nEvaluation for VGG16 Transfer Learning Model:\")\n",
    "evaluate_and_report(vgg16_transfer_model, tf_dataset_test_transfer)\n",
    "\n",
    "# Plot training and validation errors vs. epochs\n",
    "def plot_training_history(history):\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history for transfer learning models\n",
    "print(\"\\nTraining History for ResNet50 Transfer Learning Model:\")\n",
    "plot_training_history(resnet50_history)\n",
    "\n",
    "print(\"\\nTraining History for VGG16 Transfer Learning Model:\")\n",
    "plot_training_history(vgg16_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
